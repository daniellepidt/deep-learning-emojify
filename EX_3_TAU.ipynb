{"cells":[{"cell_type":"markdown","metadata":{"id":"Qj2MHAH_5cT5"},"source":["## Emojify!\n","Welcome to the third and last programming assignment! You are going to use word vector representations to build an Emojifier.\n","\n","You will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (⚾️).\n","\n","By using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set."]},{"cell_type":"markdown","metadata":{"id":"De1sZfiz5cT9"},"source":["## Packages"]},{"cell_type":"markdown","metadata":{"id":"nKZY_w-e5cT-"},"source":["Let's first import all the packages that you will need during this part of assignment.\n","\n","Feel free to use another libraries if you want to.\n","\n","If you don't have emoji or other libraries, write \"pip install emoji\" command in one of the code cells in the notebook. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#pip install emoji"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671715133204,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"1R-OcY6e5cT-"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch as torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import emoji\n","import os"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["General Params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n","batch_size = 50\n","epochs = 200\n","num_workers = 0"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["graph loss_vs_epochs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loss_vs_epochs(train_losses_list, test_losses_list):\n","    plt.figure(figsize = (15,5))\n","    plt.plot(train_losses_list, label = 'Train Loss', c = 'red')\n","    plt.plot(test_losses_list, label = 'Test Loss', c = 'blue')\n","    plt.title('Train Loss VS & Test Loss per Epoch')\n","    plt.xlabel = (\"Epoch\")\n","    plt.ylabel = (\"Loss\")\n","    #plt.ylim(0, 1)\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fzuBgLh-5cT_"},"source":["## Import and visualize the data"]},{"cell_type":"markdown","metadata":{"id":"K7o-wxwk5cUA"},"source":["In this part you need to:\n","1. Import the train and test data\n","2. Seperate the sentences (in the first column) and the index of the emoji (in the second column).\n","3. Convert the Y value of every sentence from emoji index (0-4) to one hot encoding. 2 --> [0,0,1,0,0]\n","4. Print 10 sentances from training data and visualize their matching emojies using the label_to_emoji() help function. Print also the one-hot-encoding representation of these sentences."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671715135648,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"x_mp0gfs5cUA"},"outputs":[],"source":["### START CODE HERE ###\n","#from google.colab import drive\n","#drive.mount(\"/content/drive\")\n","#path = \"/content/drive/MyDrive/Colab Notebooks\""]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671715138215,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"8V4Z-e355cUA"},"outputs":[],"source":["emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",   \n","                    \"1\": \":baseball:\",\n","                    \"2\": \":smile:\",\n","                    \"3\": \":disappointed:\",\n","                    \"4\": \":fork_and_knife:\"}\n","\n","def label_to_emoji(label):\n","    \"\"\"\n","    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n","    \"\"\"\n","    return emoji.emojize(emoji_dictionary[str(label)], language='alias')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 1. Import the train and test data\n","#train_data = pd.read_csv(io.BytesIO(uploaded['train_emoji.csv']), header=None)\n","#test_data = pd.read_csv(io.BytesIO(uploaded['test_emoji.csv']), header=None)\n","train_data = pd.read_csv(\"train_emoji.csv\", header=None)\n","test_data = pd.read_csv(\"test_emoji.csv\", header=None)\n","#train_data = pd.read_csv(path + \"/train_emoji.csv\", header=None)\n","#test_data = pd.read_csv(path + \"/test_emoji.csv\", header=None)\n","\n","# 2. Seperate the sentences (in the first column) and the index of the emoji (in the second column).\n","train_data_x, train_data_y = train_data.iloc[:, 0].to_numpy(), train_data.iloc[:, 1].to_numpy(), \n","test_data_x, test_data_y = test_data.iloc[:, 0].to_numpy(), test_data.iloc[:, 1].to_numpy(), \n","\n","# 3. Convert the Y value of every sentence from emoji index (0-4) to one hot encoding. 2 --> [0,0,1,0,0]\n","def index_to_one_hot_encoding(Y, length):\n","  return np.eye(length)[Y.reshape(-1)]\n","\n","length = 5\n","train_data_y_onehot_encoded = index_to_one_hot_encoding(train_data_y, length)\n","test_data_y_onehot_encoded = index_to_one_hot_encoding(test_data_y, length)\n","\n","# 4. Print 10 sentances from training data and visualize their matching emojies using the label_to_emoji() help function. Print also the one-hot-encoding representation of these sentences.\n","for index in range(10):\n","  print(f\"#{index}  {label_to_emoji(train_data_y[index])}  {train_data_x[index]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xH9Up7pB5cUB"},"source":["## Help functions for word embedding"]},{"cell_type":"markdown","metadata":{"id":"TmkwCoop5cUC"},"source":["The following functions will help you conver words and sentences to vectors and matrixes."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1671715140870,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"NmzokL7m5cUC"},"outputs":[],"source":["# A function that obtains vector representations for words. Each word is represented by vector with size 50.\n","# words_to_index is a dictionary that maps word into indexes - every word has a number. 'banana' --> 67752\n","# index_to_words is a dictionary that maps indexes into indexes - every index has a matching word. 344429 --> 'strawberry'\n","\n","def read_glove_vecs(glove_file):\n","    with open(glove_file, 'r',encoding='UTF-8') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","        \n","        i = 1\n","        words_to_index = {}\n","        index_to_words = {}\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1671715166978,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"K09LOc6i5cUD"},"outputs":[],"source":["# load word embeddings and create word_to_index and index_to_word dictionaries\n","\n","word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671715187308,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"0IV7R94O5cUD"},"outputs":[],"source":["# visualization: \n","\n","print(f\"The vector embedding of banana is: {word_to_vec_map['banana']}\")\n","print(f\"The index of the word 'tree' is: {word_to_index['tree']}\")\n","print(f\"The word matcing the index 173081 is: {index_to_word[173081]}\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":407,"status":"ok","timestamp":1671715180393,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"jXzo-oDe5cUE"},"outputs":[],"source":["# A function that translates the sentences vectors of word indexes --> I love you --> [185457,226278,394475]\n","# the function uses padding of the longes sentence in the train set, so I love you --> [185457,226278,394475,0,0,0,0,0,0,0]\n","\n","def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]\n","    X_indices = np.zeros((m, max_len))\n","    for i in range(m):\n","        sentence_words = X[i].lower().split()\n","        j = 0\n","        for w in sentence_words:\n","            X_indices[i, j] = int(word_to_index[w])\n","            j = j + 1\n","    return X_indices\n","\n","sentences_to_indices(np.array(['hello world']),word_to_index,4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dv-rW3DU5cUE"},"outputs":[],"source":["# function that maps all the word indexes to their vectors embedding. \n","# the embedding function is in shape (400000, 50) - each word is a vector in size 50.\n","\n","def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n","    vocab_len = len(word_to_index) + 1  #word index begin with 1,plus 1 for padding 0\n","    emb_dim = word_to_vec_map[\"cucumber\"].shape[0] # the size of embedding of each word\n","    emb_matrix = np.zeros((vocab_len, emb_dim))\n","    for word, index in word_to_index.items():\n","        emb_matrix[index, :] = word_to_vec_map[word]\n","    return emb_matrix\n"]},{"cell_type":"markdown","metadata":{"id":"HqJqHN8A5cUF"},"source":["## Train and test data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"b5gF3q5q5cUF"},"source":["The models that you will build will get the sentences as their vector representations - i.e., the sentences_to_indices() function output. \n","\n","Therefore, you need to:\n","* Transform the data to the right form using the above functions\n","* Transform the data and lables to tensors\n","* If needed, create train and test data loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HecBSiI45cUF"},"outputs":[],"source":["### START CODE HERE ###\n","# 1. Transform the data to the right form using the above functions\n","# First, find the max sentence lenght\n","longest_sentence = max(train_data_x, key=len)\n","max_sentence_len = len(longest_sentence.split())\n","\n","# Now, convert the sentences to indexes vectors by the function \"sentences_to_indices\"\n","train_data_to_indices = sentences_to_indices(train_data_x, word_to_index, max_sentence_len)\n","test_data_to_indices = sentences_to_indices(test_data_x, word_to_index, max_sentence_len)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 2. Transform the data and lables to tensors\n","# train data set\n","train_data_x_tensor = torch.from_numpy(train_data_to_indices)\n","train_data_y_tensor = torch.from_numpy(train_data_y)\n","train_data_tensor = torch.utils.data.TensorDataset(train_data_x_tensor, train_data_y_tensor)\n","\n","# train data set\n","test_data_x_tensor = torch.from_numpy(test_data_to_indices)\n","test_data_y_tensor = torch.from_numpy(test_data_y)\n","test_data_tensor = torch.utils.data.TensorDataset(test_data_x_tensor, test_data_y_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 3. create train and test data \n","#train_batch_size = len(train_data_x)\n","train_loader = torch.utils.data.DataLoader(dataset=train_data_tensor, batch_size=batch_size, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(dataset=test_data_tensor, batch_size=batch_size, num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{"id":"2BMgodE05cUF"},"source":["# First model - regular neural network"]},{"cell_type":"markdown","metadata":{"id":"KqP7DJHY5cUF"},"source":["Build a neural network model that gets:\n","\n","1. The vocabulary size\n","2. Embedding dimention - the length of every embedding vector\n","3. Pretrained embedding weights - the embedding matrix \n","                                       \n","and returns: \n","1. 5 dimention vector with the scores of every emoji.\n","\n","\n","---\n","\n","Then train the model and plot loss vs. epoch for train and test set. \n","\n","Show the results on 5 new sentences.\n","\n","\n","---\n","\n","You can use the added model as your base model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgAiouAf5cUG"},"outputs":[],"source":["### START CODE HERE ###\n","def train_model_2(device, epochs, train_loader, test_loader, model, optimizer, loss_func, train_x_tensor, test_x_tensor):\n","  \n","  train_loss_list = []\n","  train_accuracy_list = []\n","  test_loss_list = []\n","  test_accuracy_list = []\n","\n","  for e in range(epochs):\n","    train_loss_calc = 0\n","    test_loss_calc = 0\n","    train_accuracy_calc = 0\n","    test_accuracy_calc = 0 \n","\n","    # model.train()\n","\n","    for train_inputs, train_labels in train_loader:\n","      train_inputs = Variable(train_inputs.long())\n","      train_labels = Variable(train_labels.long())\n","      optimizer.zero_grad() # Clear the gradients of all optimized variables\n","      train_outputs = model(train_inputs) # Forward pass\n","      loss = loss_func(train_outputs, train_labels) # Calculate the loss\n","      loss.backward()\n","      optimizer.step() # Doing the optimizer step\n","      train_loss_calc += loss.item() # Update running training loss\n","      model.train() # Actually training the net\n","      for i in range(len(train_inputs)):\n","        train_original_index = train_labels[i]\n","        train_predictive_index = np.argmax(train_outputs[i].detach().numpy())\n","        if train_original_index == train_predictive_index:\n","          train_accuracy_calc += 1\n","\n","    # Update lists  \n","    train_loss_list.append(train_loss_calc/len(train_loader.dataset))\n","    train_accuracy_list.append(train_accuracy_calc/len(train_x_tensor))    \n","      \n","    # model.eval()\n","      \n","    with torch.no_grad():\n","      for test_input, test_labels in test_loader:\n","        test_input = Variable(test_input.long())\n","        test_labels = Variable(test_labels.long())\n","        test_outputs = model(test_input) # Forward pass\n","        test_loss_calc += loss_func(test_outputs, test_labels).item() # Update running training loss\n","        for i in range(len(test_input)):\n","          test_original_index = test_labels[i]\n","          test_predictive_index = np.argmax(test_outputs[i].detach().numpy())\n","          #a =np.argmax(pred_2[i])\n","          if test_original_index == test_predictive_index:\n","            test_accuracy_calc += 1\n","\n","      test_loss_list.append(test_loss_calc/len(test_loader.dataset))\n","      test_accuracy_list.append(test_accuracy_calc/len(train_x_tensor))      \n","\n","    if e % 10 == 0:\n","      print(f\"Epoch: {e + 1}/{epochs}... \\tTrain Loss: {round(train_loss_calc, 5)} \\tTrain Accuracy: {round(train_accuracy_calc, 5)} \\tTest Loss: {round(test_loss_calc, 5)} \\tTest Accuracy: {round(test_accuracy_calc, 5)}\")\n","\n","  return train_loss_list, train_accuracy_list, test_loss_list, test_loss_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yh8nfDsa5cUG"},"outputs":[],"source":["class NN_Model(nn.Module):\n","    \n","    def __init__(self,vocab_size,embedding_dim,pretrained_weight):\n","        super(NN_Model,self).__init__()\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim) # stores embeddings of a fixed dictionary and size\n","        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight)) # place the pretrained weights to the embedding function\n","        self.layers = nn.Sequential(\n","        nn.Linear(embedding_dim, embedding_dim//2), \n","        nn.ReLU(),\n","        nn.Linear(embedding_dim//2, embedding_dim//4),\n","        nn.ReLU(),\n","        nn.Linear(embedding_dim//4, 5),\n","        nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self,x):\n","        out = self.word_embeds(x) \n","        # out = torch.flatten(out, start_dim=1)\n","        out = out[:,-1,:]\n","        out = self.layers(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"loawPDFv5cUG"},"source":["# Second model - neural network with RNN"]},{"cell_type":"markdown","metadata":{"id":"BLDhHs3Y5cUG"},"source":["Build a neural network + RNN model that gets the vocabulary size, embedding dimention and pretrained embedding weights, and returns a 5 dimention vector with the scores of every emoji.\n","\n","Then train the model and plot loss vs. epoch for train and test set.\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","---\n","\n","You can use the added model as your base model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aExAS2vv5cUG"},"outputs":[],"source":["### START CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQn3irlb60Yi"},"outputs":[],"source":["class RNN_Model(nn.Module):\n","    \n","    def __init__(self,vocab_size,embedding_dim,pretrained_weight):\n","        super(RNN_Model,self).__init__()\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim) # stores embeddings of a fixed dictionary and size\n","        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight)) # place the pretrained weights to the embedding function\n","        self.rnn = ### add the model here\n","        self.layers = nn.Sequential(\n","        ### add the model here\n","        )\n","\n","    def forward(self,x,h):\n","        out = self.word_embeds(x)\n","        out, _ = self.rnn(out,h)\n","        out = out[:, -1, :]\n","        out = self.layers(out)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"pyGh1SVB78ur"},"source":["# Third model - neural network with transformers"]},{"cell_type":"markdown","metadata":{"id":"0ko-FP1978ur"},"source":["Build a neural network + transformer model that gets the vocabulary size, embedding dimention and pretrained embedding weights, and returns a 5 dimention vector with the scores of every emoji.\n","\n","Then train the model and plot loss vs. epoch for train and test set.\n","\n","https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n","\n","\n","---\n","\n","You can use the added model as your base model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbO1izqW78ur"},"outputs":[],"source":["### START CODE HERE ###"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":309,"status":"ok","timestamp":1671715302552,"user":{"displayName":"ליה גורביץ","userId":"09515891463711318510"},"user_tz":-120},"id":"O_HZJmZN8Shu"},"outputs":[],"source":["import math\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0G4Y_Fq78us"},"outputs":[],"source":["class Transformer_model(nn.Module):\n","    \n","    def __init__(self,vocab_size, embedding_dim, pretrained_weight):\n","        super(Transformer_model,self).__init__()\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))\n","\n","        self.pos_encoding = PositionalEncoding(embedding_dim)\n","        self.layers = nn.Sequential(\n","            ### complete code here\n","        )\n","\n","\n","    def forward(self,x):\n","        out = self.word_embeds(x)\n","        out += self.pos_encoding(out)\n","        out = self.layers(out)\n","        return out\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ApF5nFs5cUH"},"source":["### Compare between the models - who had the best results? Try to explain why. "]},{"cell_type":"markdown","metadata":{"id":"pxEIIjmS5cUH"},"source":["Write your answere here"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
